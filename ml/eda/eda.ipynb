{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GitHub Pull Requests: EDA Notebook\n",
    "\n",
    "This notebook shows how to:\n",
    "1. Load raw GitHub PR JSON data.\n",
    "2. Flatten the nested data structure into a Pandas DataFrame.\n",
    "3. Subset columns to keep only those we care about.\n",
    "4. Perform basic exploratory data analysis (EDA).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Imports\n",
    "\n",
    "I begin by importing our required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Flatten the PR JSON Data\n",
    "\n",
    "I already have a file named `github_prs_raw.json` inside the `../data/` folder. Let's open it, parse the JSON, and create a DataFrame using `pd.json_normalize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 305\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7cb60d204e53>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total columns: {df_all.shape[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just show the head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# Open the raw PR data from a JSON file\n",
    "with open(\"../data/github_prs_raw.json\", \"r\") as f:\n",
    "    raw_pr_data = json.load(f)\n",
    "\n",
    "# Flatten the nested JSON structure\n",
    "df_all = pd.json_normalize(raw_pr_data)\n",
    "\n",
    "# Let's see how many columns we have and show a preview\n",
    "print(f\"Total columns: {df_all.shape[1]}\")\n",
    "df_all.head()  # Just show the head\n",
    "df_all.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect All Column Names\n",
    "\n",
    "I'll list out all the flattened columns to decide which ones are relevant for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = df_all.columns.tolist()\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Columns\n",
    "\n",
    "Here, I can see columns like `number`, `state`, `title`, `body`, `merged_at`, etc.  \n",
    "For a simpler EDA, let's select a small set of columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Select Key Columns\n",
    "\n",
    "I'll define a list of columns that are present and relevant (based on our earlier inspection). I then create a smaller DataFrame `df` with just those columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick columns you actually have in df_all\n",
    "selected_columns = [\n",
    "    \"number\",        # PR number\n",
    "    \"state\",         # open/closed\n",
    "    \"title\",         # PR title\n",
    "    \"body\",          # PR description\n",
    "    \"created_at\",    # Creation timestamp\n",
    "    \"updated_at\",    # Last updated timestamp\n",
    "    \"closed_at\",     # If closed\n",
    "    \"merged_at\",     # If merged\n",
    "    \"assignee\",      # Single assignee (nullable)\n",
    "    \"assignees\",     # List of assigned users\n",
    "    \"labels\",        # Label objects\n",
    "    \"user.login\"     # Author's username\n",
    "]\n",
    "\n",
    "# Ensure we only include columns that exist\n",
    "existing_cols = [col for col in selected_columns if col in df_all.columns]\n",
    "\n",
    "df = df_all[existing_cols].copy()\n",
    "\n",
    "print(f\"Columns in df: {df.columns.tolist()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Missing Data\n",
    "\n",
    "Certain columns (like `assignee` or `assignees`) might be empty or `None` across most rows. Let's plot a heatmap of missing values for a subset of the columns (e.g., the first 50) to see the overall pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# We'll just take the first 50 columns to avoid crowding\n",
    "subset = df.iloc[:, :50]\n",
    "\n",
    "sns.heatmap(subset.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Missing Values Heatmap (First 50 Columns)\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Drop Columns with Excessive Missing Values\n",
    "\n",
    "From the heatmap, I see `assignee` is nearly always `None`. Let's remove it (and any other columns that meet a certain missing threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on a threshold; e.g., drop columns missing in over 90% of rows\n",
    "threshold = 0.8\n",
    "\n",
    "# Calculate the percentage of NaN in each column\n",
    "missing_percent = df.isnull().mean()\n",
    "\n",
    "# Filter columns that exceed our threshold\n",
    "cols_to_drop = missing_percent[missing_percent > threshold].index\n",
    "print(\"Dropping columns:\", cols_to_drop.tolist())\n",
    "\n",
    "# Drop them\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Confirm theyâ€™re gone\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def empty_list_to_nan(x):\n",
    "    # If x is a list and it's empty, return NaN;\n",
    "    # otherwise return x as-is.\n",
    "    if isinstance(x, list) and len(x) == 0:\n",
    "        return np.nan\n",
    "    return x\n",
    "\n",
    "# Drop the 'assignees' column entirely\n",
    "df.drop(columns=\"assignees\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Double-check that 'assignees' is gone\n",
    "print(\"Columns after drop:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Drop Missing Times\n",
    "\n",
    "If a pull request is never merged, or if `merged_at` is null, I can't calculate a valid time-to-merge. Let's remove rows where the merge time is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert created_at / merged_at to datetime if not already done\n",
    "df[\"created_dt\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
    "df[\"merged_dt\"] = pd.to_datetime(df[\"merged_at\"], errors=\"coerce\")\n",
    "\n",
    "# Compute time_to_merge_hours\n",
    "df[\"time_to_merge_hours\"] = (df[\"merged_dt\"] - df[\"created_dt\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# Some PRs might never merge; let's drop rows missing this\n",
    "before_drop = len(df)\n",
    "df.dropna(subset=[\"time_to_merge_hours\"], inplace=True)\n",
    "after_drop = len(df)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows where 'time_to_merge_hours' was NaN.\")\n",
    "df[[\"number\", \"state\", \"time_to_merge_hours\"]].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df_final is your cleaned and processed DataFrame in your EDA notebook\n",
    "df[[\"number\", \"title\", \"time_to_merge_hours\"]].to_csv(\"/Users/cememirsenyurt/github-pr-time-estimator/ml/data/processed_pr_data.csv\", index=False)\n",
    "print(\"Processed data saved to ../data/processed_pr_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
